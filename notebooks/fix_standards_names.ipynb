{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pubchempy as pcp\n",
    "from tqdm.notebook import tqdm\n",
    "import re\n",
    "import pickle\n",
    "import os\n",
    "from termcolor import colored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "polarity = \"negative\"\n",
    "missing_name = \"Undefined\"\n",
    "offenders = (\"nan\", \"Untitled\", \"Oprea\", \"Opera\", \"AKO\", \"CHEMBL\", \"SR-\", \"SCHEM\", \"EU-\", \"MLS\", \"NSC\", \"ChemDiv\", \"ST0\", \"TimTec\", \"HMS\", \"BIM\", \"CB\", \"CCG-\", \"Cambridge\", \"SMR\", \"AB0\", \"BRD-\", \"NCG\", \"BDBM\", \"CBKinase\", \"BAS \", \"ZINC\", \"GNF\", \"SQX\", \"CDS\", \"STK\", \"NCI\", \"TNP\", \"Boc-Tyr-OH\", \"PD\", \"UNM\", \"BSP\", \"CCRIS\", \"MFCD\", \"IDI\", \"ST5\", \"AC1\", \"WAY-\", \"KUC\", \"DTXSID\", \"MixCom\", \"CK-\", \"ASN \", \"MMV\", \"SKI-\", \"VU\", \"SMSF\", \"Bio2\", \"REGID\", \"SDCC\", \"BCBc\", \"SMP\", \"TCMDC\", \"cid_\", \"BCP\", \"AST \", \"SY0\", \"AM-\", \"IFLab\")\n",
    "save_pubchem_dict = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "c18_standards_df = pd.read_csv('./C18_standards_' + polarity + '.tsv', sep='\\t', float_precision='round_trip')\n",
    "inchikeys = c18_standards_df['inchi_key']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile('C18_standards_' + polarity + '_all-adducts_pubchem_dict.pkl'):\n",
    "    \n",
    "    print (\"PubChem dictionary already exists in working directory - exiting\")\n",
    "    \n",
    "else:\n",
    "\n",
    "    inchikey_to_iupac = {}\n",
    "\n",
    "    for key in tqdm(inchikeys):\n",
    "\n",
    "        #print(\"Gathering data for \" + key)\n",
    "\n",
    "        if key in inchikey_to_iupac.keys():\n",
    "            pass\n",
    "        else:\n",
    "            try:\n",
    "                # Run to PubChem and look for InChI key\n",
    "                cid = pcp.get_compounds(key, namespace='inchikey', as_dataframe=True, listkey_count=5).reset_index()['cid'].to_string(index=False)\n",
    "                \n",
    "                # Define compound properties\n",
    "                if \"\\n\" in cid:\n",
    "                    cid = (cid.rstrip().split('\\n'))[-1]\n",
    "                else:\n",
    "                    cid = cid\n",
    "                compound = pcp.Compound.from_cid(cid)\n",
    "                iupac_name = compound.iupac_name\n",
    "                synonym_list = compound.synonyms\n",
    "                \n",
    "                # Check properties and assign to compound based on presence/absence in DB\n",
    "                if not iupac_name:\n",
    "                    iupac_name = missing_name\n",
    "                if not synonym_list:\n",
    "                    synonyms = missing_name\n",
    "                else:\n",
    "                    synonyms = synonym_list\n",
    "                \n",
    "                full_name = synonyms\n",
    "\n",
    "            # If InChI key doesn't exist, create an undefined named that matches format\n",
    "            except KeyError:\n",
    "                \n",
    "                print(\"Note: \" + key + \" is not found in PubChem!\")\n",
    "\n",
    "                full_name = missing_name\n",
    "            \n",
    "            inchikey_to_iupac[key] = (full_name)\n",
    "\n",
    "    if save_pubchem_dict == True:\n",
    "        \n",
    "        filehandler = open('C18_standards_' + polarity + '_all-adducts_pubchem_dict.pkl', 'wb')\n",
    "        pickle.dump(inchikey_to_iupac, filehandler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorts through the PubChem synonym list and chooses the best name\n",
    "\n",
    "def fix_synonym_list(synonym_dict: dict) -> dict:\n",
    "\n",
    "    for inchi_key in tqdm(synonym_dict):\n",
    "            \n",
    "        synonym_subset = synonym_dict[inchi_key]\n",
    "\n",
    "        if isinstance(synonym_subset, str) == True:\n",
    "            synonym_subset = [synonym_subset]\n",
    "\n",
    "        synonym_subset = [x for x in synonym_subset if not x.startswith(offenders)]\n",
    "        synonym_subset = list(filter(lambda x: not x.replace(\"-\", \"\").replace(re.compile('^A-Z').pattern, \"\").isdigit(), synonym_subset))\n",
    "        synonym_subset = list(filter(lambda x: not re.sub(r'\\b[A-Z]{1}',\"\",x).isdigit(), synonym_subset))\n",
    "        synonym_subset = [x for x in synonym_subset if not re.compile(r'\\b[A-Z]{14}\\b-\\b[A-Z]{10}\\b-\\b[A-Z]{1}\\b').search(x)]\n",
    "        synonym_subset = [x for x in synonym_subset if not re.compile(r'^F\\d{4}-\\d{4}').search(x)]\n",
    "\n",
    "        if not synonym_subset:\n",
    "            top_synonym = missing_name\n",
    "        else:\n",
    "            top_synonym = synonym_subset[0]\n",
    "\n",
    "        synonym_dict[inchi_key] = top_synonym\n",
    "\n",
    "    return synonym_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose best synonym for each InChI key and write new dictionary (with single str as value instead of list)\n",
    "\n",
    "with open('C18_standards_' + polarity + '_all-adducts_pubchem_dict.pkl', 'rb') as filehandler:\n",
    "\n",
    "    pubchem_dictionary = pickle.load(filehandler)\n",
    "\n",
    "    relabel_dict = fix_synonym_list(pubchem_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix 'label' column to remove internal \"number: \" and \"adduct\" features\n",
    " \n",
    "c18_standards_df_relabel = c18_standards_df.copy()\n",
    "\n",
    "c18_standards_df_relabel['label'] = c18_standards_df_relabel['label'].apply(lambda x: pd.Series(x.split(' ', 1)))[1].apply(lambda x: pd.Series(re.split(r' \\[M', x)))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 'label' and 'name' dictionaries to fix those too\n",
    "\n",
    "label_dict = c18_standards_df_relabel.set_index('inchi_key')['label'].to_dict()\n",
    "\n",
    "name_dict = c18_standards_df_relabel.set_index('inchi_key')['name'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixes the existing label and names column\n",
    "\n",
    "def fix_label_and_name(input_dict:dict) -> dict:\n",
    "\n",
    "    for inchi_key, value_string in tqdm(input_dict.items()):\n",
    "\n",
    "        new_value_string = re.sub(r'\\b[A-Z]{14}\\b-\\b[A-Z]{10}\\b-\\b[A-Z]{1}\\b', 'Undefined', value_string)\n",
    "        new_value_string = re.sub(r'^F\\d{4}-\\d{4}', 'Undefined', new_value_string)\n",
    "        if new_value_string.startswith(offenders):\n",
    "            new_value_string = 'Undefined'\n",
    "        if new_value_string.replace(\"-\", \"\").isdigit():\n",
    "            new_value_string = 'Undefined'\n",
    "\n",
    "        input_dict[inchi_key] = new_value_string\n",
    "    \n",
    "    return input_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all three name dictionaries in one place and check lengths\n",
    "\n",
    "label_dict_update = fix_label_and_name(label_dict)\n",
    "name_dict_update = fix_label_and_name(name_dict)\n",
    "relabel_dict_update = relabel_dict\n",
    "\n",
    "if len(label_dict_update.values()) == len(name_dict_update.values()) == len(relabel_dict_update.values()):\n",
    "    print(\"All dictionaries have \" + str(len(label_dict_update.keys())) + \" keys. Good to go!\")\n",
    "else:\n",
    "    print(\"Warning! Dictionaries don't have the same number of keys.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual curation is required for some edge cases\n",
    "\n",
    "def choose_parsimonious_label(multi_name_df:pd.DataFrame) -> pd.DataFrame:\n",
    "    \n",
    "    multi_name_df_singled = multi_name_df.copy()\n",
    "    multi_name_df_singled = multi_name_df_singled.replace(\"Undefined\", (\"Undefined\"*50)) # Create artificially long Undefined string\n",
    "    multi_name_df_singled['combo'] = multi_name_df_singled['relabel'] + \";;\" + multi_name_df_singled['name'] + \";;\" + multi_name_df_singled['label']\n",
    "    multi_name_df_singled['combo'] = multi_name_df_singled['combo'].str.lower()\n",
    "    multi_name_df_singled['new_label'] = multi_name_df_singled.combo.str.split(';;').apply(lambda x: min(x, key=len))\n",
    "\n",
    "    if(polarity == \"negative\"):\n",
    "        multi_name_df_singled['new_label'] = multi_name_df_singled['new_label'].str.replace(\"cape [m-h]-\",\"caffeic acid phenethyl ester [m-h]-\")\n",
    "        multi_name_df_singled['new_label'] = multi_name_df_singled['new_label'].str.replace(\"2',4'-dihydroxyacetophenone [m-h]-\",\"resacetophenone [m-h]-\")\n",
    "        multi_name_df_singled['new_label'] = multi_name_df_singled['new_label'].str.replace(\"egcg [m-h]-\",\"epigallocatechin gallate [m-h]-\")\n",
    "        multi_name_df_singled['new_label'] = multi_name_df_singled['new_label'].str.replace(\"fmet [m-h]-\",\"n-formyl-l-methionine [m-h]-\")\n",
    "\n",
    "    if(polarity == \"positive\"):\n",
    "        multi_name_df_singled['new_label'] = multi_name_df_singled['new_label'].str.replace(\"c27h32fno2 [m+h]+\",\"(2E)-1-[2-(4-fluorophenyl)-6-hydroxy-3-azabicyclo[4.4.0]dec-3-yl]-3-[4-(methyl ethyl)phenyl]prop-2-en-1-one [m+h]+\")\n",
    "        multi_name_df_singled['new_label'] = multi_name_df_singled['new_label'].str.replace(\"cobadex [m+h]+\",\"cortisol [m+h]+\")\n",
    "        multi_name_df_singled['new_label'] = multi_name_df_singled['new_label'].str.replace(\"trp-gly [m+h]+\",\"tryptophylglycine [m+h]+\")\n",
    "        multi_name_df_singled['new_label'] = multi_name_df_singled['new_label'].str.replace(\"c276-0086 [m+h]+\",\"4-(4-hydroxy-3,5-dimethoxyphenyl)-6,7-dimethoxy-1,3,4-trihydroquinolin-2-one [m+h]+\")\n",
    "        multi_name_df_singled['new_label'] = multi_name_df_singled['new_label'].str.replace(\"antibiotic 9663 [m+h]+\",\"ochratoxin a [m+h]+\")\n",
    "        multi_name_df_singled['new_label'] = multi_name_df_singled['new_label'].str.replace(\"sw219134-1 [m+h]+\",\"silymarin [m+h]+\")\n",
    "        multi_name_df_singled['new_label'] = multi_name_df_singled['new_label'].str.replace(\"megxp0_000848 [m+h]+\",\"haploperoside c [m+h]+\")\n",
    "        multi_name_df_singled['new_label'] = multi_name_df_singled['new_label'].str.replace(\"Œ± [m+h]+\",\"alpha [m+h]+\")\n",
    "        multi_name_df_singled['new_label'] = multi_name_df_singled['new_label'].str.replace(\".alpha. [m+h]+\",\"alpha [m+h]+\")\n",
    "        multi_name_df_singled['new_label'] = multi_name_df_singled['new_label'].str.replace(\"egcg [m-h]-\",\"epigallocatechin gallate [m-h]-\")\n",
    "        multi_name_df_singled['new_label'] = multi_name_df_singled['new_label'].str.replace(\"minocycline¬∑hcl [m-h]-\",\"minocycline hydrochloride [m-h]-\")\n",
    " \n",
    "    return multi_name_df_singled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the three dictionaries and choose shortest string (likely to be the best)\n",
    "\n",
    "combined_dictionaries = pd.DataFrame({'relabel':pd.Series(relabel_dict_update),'name':pd.Series(name_dict_update),'label':pd.Series(label_dict_update)})\n",
    "\n",
    "new_label_df = choose_parsimonious_label(combined_dictionaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert new label into original file\n",
    "\n",
    "c18_standards_df_updated = c18_standards_df.copy()\n",
    "new_label_df_updated = new_label_df.copy()\n",
    "\n",
    "new_label_df_updated['inchi_list'] = new_label_df_updated.index\n",
    "new_label_df_updated = new_label_df_updated[['inchi_list','new_label']]\n",
    "new_label_df_updated = new_label_df_updated.reset_index(drop=True)\n",
    "new_label_df_updated\n",
    "\n",
    "original_and_updated_merge = pd.merge(c18_standards_df_updated, new_label_df_updated, right_on='inchi_list', left_on='inchi_key', how='left')\n",
    "original_and_updated_merge.insert(3, \"new_label\", original_and_updated_merge.pop('new_label'))\n",
    "original_and_updated_merge = original_and_updated_merge.drop(original_and_updated_merge[['inchi_list','label']], axis=1)\n",
    "original_and_updated_merge = original_and_updated_merge.rename(columns = {'new_label': 'label'})\n",
    "\n",
    "if original_and_updated_merge.shape == c18_standards_df.shape:\n",
    "    print(\"Original and updated dataframes have the same dimensions of \" + str(original_and_updated_merge.shape) + \". Good to go!\")\n",
    "else:\n",
    "    print(\"Warning! Original and updated dataframes have different dimensions.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write new standards file with new label substituted\n",
    "\n",
    "original_and_updated_merge.to_csv('C18_standards_' + polarity + '_all-adducts_renamed.tsv', sep='\\t', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all duplicated inchi_keys (with the same adduct)\n",
    "\n",
    "original_and_updated_merge['inchi-adduct'] = original_and_updated_merge['inchi_key'].astype(str) + original_and_updated_merge['adduct']\n",
    "duplicated_inchis = list(original_and_updated_merge[original_and_updated_merge['inchi-adduct'].duplicated()]['inchi-adduct'].drop_duplicates().values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset label-fixed standards file by only duplicated inchi keys\n",
    "\n",
    "c18_standards_df_dups = original_and_updated_merge[original_and_updated_merge['inchi-adduct'].isin(duplicated_inchis)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate retention time deviations and print to file\n",
    "\n",
    "duplicate_summary_stats = []\n",
    "\n",
    "for inchi in tqdm(duplicated_inchis):\n",
    "\n",
    "    subset_df = c18_standards_df_dups.loc[c18_standards_df_dups['inchi-adduct'] == inchi]\n",
    "    inchi_key = subset_df['inchi_key'].tolist()[0]\n",
    "    inchi_name = subset_df['label'].tolist()[0]\n",
    "    inchi_adduct = subset_df['adduct'].tolist()[0]\n",
    "    inchi_count = len(subset_df)\n",
    "    inchi_mean = subset_df['rt_peak'].mean()\n",
    "    inchi_std = subset_df['rt_peak'].std()\n",
    "    inchi_files = subset_df['file_name'].tolist()\n",
    "\n",
    "    duplicate_summary_stats.append([inchi, inchi_key, inchi_name, inchi_adduct, inchi_count, inchi_mean, inchi_std, inchi_files])\n",
    "\n",
    "duplicate_summary_table = pd.DataFrame(duplicate_summary_stats)\n",
    "duplicate_summary_table.rename(columns={0: 'inchi-adduct', 1: 'inchi_key', 2: 'label', 3: 'adduct', 4: 'duplications', 5: 'rt_peak_mean', 6: 'rt_peak_std', 7: 'file_names'}, inplace=True)\n",
    "duplicate_summary_table.sort_values('rt_peak_std', ascending=False, inplace=True)\n",
    "\n",
    "duplicate_summary_table.to_csv('C18_standards_' + polarity + '_all-adducts_duplicate_standards.tsv', sep='\\t', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose which duplicated row will be retained with a decision tree\n",
    "\n",
    "vetted_standards = pd.read_excel(\"top_two_per_polarity_from_drive.xlsx\")\n",
    "vetted_standards = vetted_standards[vetted_standards['polarity'] == polarity]\n",
    "\n",
    "deduplicated_data = []\n",
    "decision_dict = {}\n",
    "\n",
    "for dupkey, adduct in tqdm(list(zip(duplicate_summary_table['inchi_key'], duplicate_summary_table['adduct']))):\n",
    "    \n",
    "    dupkey_df = original_and_updated_merge[(original_and_updated_merge['inchi_key'] == dupkey) & (original_and_updated_merge['adduct'] == adduct)]\n",
    "    entries = dupkey_df.shape[0]\n",
    "\n",
    "    print(dupkey + \" \" + adduct + \" (\" + str(entries) + \" dups)\")\n",
    "\n",
    "    ## Decision tree:\n",
    "\n",
    "    if vetted_standards['inchi_key'].str.contains(dupkey).any():\n",
    "\n",
    "        top_hit = vetted_standards[(vetted_standards['inchi_key'] == dupkey) & (vetted_standards['adduct'] == adduct)]\n",
    "\n",
    "        if 0 < top_hit.shape[0] <= 2:\n",
    "\n",
    "            top_hit.sort_values('code_rank_without_msms', ascending = True)\n",
    "            top_hit = top_hit.head(1)['file_name'].values[0]\n",
    "\n",
    "            best_dupkey = dupkey_df[(dupkey_df['inchi_key'] == dupkey) & (dupkey_df['file_name'] == top_hit)]\n",
    "\n",
    "            print(colored('Filtered: ', 'green') + 'Top hits table\\n')\n",
    "            deduplicated_data.append(best_dupkey)\n",
    "            decision_dict[dupkey] = 'Top_hits_table'\n",
    "            continue\n",
    "\n",
    "        # else:\n",
    "            \n",
    "        #     print(colored('Warning: ', 'red') + 'Multiple compounds with identical inchikey')\n",
    "        #     print(top_hit['label'].values)\n",
    "        #     print('\\n')\n",
    "        #     continue\n",
    "\n",
    "    if sum(dupkey_df['valid_msms']) == 1: ## Only single True\n",
    "        \n",
    "        best_dupkey = dupkey_df[dupkey_df['valid_msms'] == True]\n",
    "        \n",
    "        print(colored('Filtered: ', 'green') + 'Valid MSMS\\n')\n",
    "        deduplicated_data.append(best_dupkey)\n",
    "        decision_dict[dupkey] = 'Valid_msms'\n",
    "        continue\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        if sum(dupkey_df.notnull()['metatlas_score'].values) > 0: # If metatlas score exists, use the highest one\n",
    "                \n",
    "            if len(dupkey_df['metatlas_score'].unique()) != 1:\n",
    "\n",
    "                dupkey_df.sort_values('metatlas_score', ascending = False)\n",
    "                best_dupkey = dupkey_df.head(1)\n",
    "\n",
    "                print(colored('Filtered: ', 'green') + 'Metatlas score\\n')\n",
    "                deduplicated_data.append(best_dupkey)\n",
    "                decision_dict[dupkey] = 'Metatlas_score'\n",
    "                continue\n",
    "\n",
    "        if \"Platinum\" in str(dupkey_df['top_two_by_rank'].value_counts().index) and dupkey_df['top_two_by_rank'].value_counts()['Platinum ' + polarity + ' max rank'] == 1: # Only one row with platinum confidence\n",
    "\n",
    "            best_dupkey = dupkey_df[dupkey_df['top_two_by_rank'] == 'Platinum ' + polarity + ' max rank']\n",
    "\n",
    "            print(colored('Filtered: ', 'green') + 'Confidence category\\n')\n",
    "            deduplicated_data.append(best_dupkey)\n",
    "            decision_dict[dupkey] = 'Platinum_confidence'\n",
    "            continue\n",
    "\n",
    "        if len(dupkey_df['code_rank_without_msms'].unique()) != 1: # If code rank exists, use the best one\n",
    "\n",
    "            dupkey_df.sort_values('code_rank_without_msms', ascending = True)\n",
    "            best_dupkey = dupkey_df.head(1)\n",
    "\n",
    "            print(colored('Filtered: ', 'green') + 'Code rank\\n')\n",
    "            deduplicated_data.append(best_dupkey)\n",
    "            decision_dict[dupkey] = 'Code_rank_integer'\n",
    "            continue\n",
    "\n",
    "        if sum(dupkey_df.notnull()['nist_score'].values) > 0: # If NIST score exists, use the highest one\n",
    "                \n",
    "            if len(dupkey_df['nist_score'].unique()) != 1:\n",
    "                \n",
    "                dupkey_df.sort_values('nist_score', ascending = False)\n",
    "                best_dupkey = dupkey_df.head(1)\n",
    "\n",
    "                print(colored('Filtered: ', 'green') + 'NIST score\\n')\n",
    "                deduplicated_data.append(best_dupkey)\n",
    "                decision_dict[dupkey] = 'NIST_score'\n",
    "                continue\n",
    "            \n",
    "        if sum(dupkey_df.notnull()['msms_score'].values) > 0: # If MSMS score exists, use the highest one\n",
    "                \n",
    "            if len(dupkey_df['msms_score'].unique()) != 1:\n",
    "                \n",
    "                dupkey_df.sort_values('msms_score', ascending = False)\n",
    "                best_dupkey = dupkey_df.head(1)\n",
    "\n",
    "                print(colored('Filtered: ', 'green') + 'MSMS score\\n')\n",
    "                deduplicated_data.append(best_dupkey)\n",
    "                decision_dict[dupkey] = 'MSMS_score'\n",
    "                continue\n",
    "\n",
    "        if sum(dupkey_df['peak_height_ms1-summary_205060'].values) + sum(dupkey_df['peak_height_ms1-summary_102040'].values) > 0: # Choose entry with largest peak heights in both energies\n",
    "            \n",
    "            sorted_indices = (dupkey_df[\"peak_height_ms1-summary_102040\"] + dupkey_df[\"peak_height_ms1-summary_205060\"]).sort_values().index\n",
    "            dupkey_df.loc[sorted_indices, :]\n",
    "            best_dupkey = dupkey_df.head(1)\n",
    "\n",
    "            print(colored('Filtered: ', 'green') + 'Peak heights\\n')\n",
    "            deduplicated_data.append(best_dupkey)\n",
    "            decision_dict[dupkey] = 'Peak_heights'\n",
    "            continue\n",
    "\n",
    "        if \"multiple peaks\" in str(dupkey_df['Peak shape'].value_counts().index) and dupkey_df['Peak shape'].value_counts()['multiple peaks'] == dupkey_df.shape[0]-1: # If there is only one row without 'multiple peaks', use it\n",
    "            \n",
    "            best_dupkey = dupkey_df[dupkey_df['Peak shape'] != 'multiple peaks']\n",
    "\n",
    "            print(\"Filtered by peak shape!\")\n",
    "            deduplicated_data.append(best_dupkey)\n",
    "            decision_dict[dupkey] = 'Peak_shape'\n",
    "            \n",
    "        else:\n",
    "\n",
    "            print(colored('Warning: ', 'red') + 'Unresolved decision tree\\n')\n",
    "            decision_dict[dupkey] = 'Unresolved'\n",
    "\n",
    "decision_list = list(decision_dict.values())\n",
    "{i:decision_list.count(i) for i in set(decision_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine clean file without duplicates and the deduplicated df\n",
    "\n",
    "deduplicated_df = pd.concat(deduplicated_data)\n",
    "original_and_updated_merge_no_duplicates = original_and_updated_merge[~original_and_updated_merge['inchi-adduct'].isin(duplicated_inchis)]\n",
    "\n",
    "new_deduplicated_standards = pd.concat([deduplicated_df, original_and_updated_merge_no_duplicates], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write new deduplicated df\n",
    "\n",
    "new_deduplicated_standards.to_csv('C18_standards_' + polarity + '_all-adducts_renamed_deduplicated.tsv', sep='\\t', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
